{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Topic_Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1q8WarmoE7q-tepTc1EKshTm0_kP-Am1P",
      "authorship_tag": "ABX9TyPl7Vx2QwofVrqwWyvGRL5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrakhshanda/Bayesian-Methods/blob/master/Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqm73ydAruzP"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaB89M6AohzS",
        "outputId": "87fdadef-bc65-4901-9942-12c4867dab31"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "\r\n",
        "import regex as re\r\n",
        "from nltk.stem.porter import PorterStemmer \r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize,RegexpTokenizer\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "from os.path import isfile, join\r\n",
        "\r\n",
        "#for vectorizing\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.stem.porter import *\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "stemmer = SnowballStemmer(\"english\")\r\n",
        "\r\n",
        "import gensim.corpora as corpora\r\n",
        "from pprint import pprint\r\n",
        "import gensim\r\n",
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "import pyLDAvis.gensim\r\n",
        "import pickle \r\n",
        "import pyLDAvis\r\n",
        "\r\n",
        "#comment this out if you're not able to view any of the print lines\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f-KunQjzmBB"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "ljxzmJ0TqAVg",
        "outputId": "4c9130ba-98fa-4dae-d395-65e771640f93"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data/data.zip', compression='zip', header=0)\r\n",
        "df = df.loc[:,~df.columns.str.match(\"Unnamed\")]\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33581499</td>\n",
              "      <td>PCNA, a focus on replication stress and the al...</td>\n",
              "      <td>The maintenance of telomeres, which are specia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33581339</td>\n",
              "      <td>Compositional Variability and Mutation Spectra...</td>\n",
              "      <td>COVID-19 and its causative pathogen SARS-CoV-2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33580188</td>\n",
              "      <td>Histone variant H2A.B-H2B dimers are spontaneo...</td>\n",
              "      <td>H2A.B is an evolutionarily distant histone H2A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33580181</td>\n",
              "      <td>Hypermutated phenotype in gliosarcoma of the s...</td>\n",
              "      <td>Gliosarcoma is a variant of glioblastoma with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33580144</td>\n",
              "      <td>Arsenic hexoxide has differential effects on c...</td>\n",
              "      <td>Arsenic is reportedly a biphasic inorganic com...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PMID  ...                                           Abstract\n",
              "0  33581499  ...  The maintenance of telomeres, which are specia...\n",
              "1  33581339  ...  COVID-19 and its causative pathogen SARS-CoV-2...\n",
              "2  33580188  ...  H2A.B is an evolutionarily distant histone H2A...\n",
              "3  33580181  ...  Gliosarcoma is a variant of glioblastoma with ...\n",
              "4  33580144  ...  Arsenic is reportedly a biphasic inorganic com...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apMOukKi0CF8"
      },
      "source": [
        "df['Text'] = df['Title'] + '. ' + df['Abstract']\r\n",
        "df['Text'] = df['Text'].astype('str')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7x5bzZ9166t"
      },
      "source": [
        "# Reuseable Routiens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC3Sbdkb1-mk"
      },
      "source": [
        "def save_object(path, filename, obj):\r\n",
        "    print('Saving Object')\r\n",
        "    path_file = join(path, filename)    \r\n",
        "    pickle.dump(obj, open(path_file, 'wb'))\r\n",
        "    print('Save complete')\r\n",
        "\r\n",
        "def load_object(path, filename):\r\n",
        "    print('Attempting to Load Object')\r\n",
        "    path_file = join(path, filename)    \r\n",
        "    obj = pickle.load(open(path_file, \"rb\" ))\r\n",
        "    print('Load complete')\r\n",
        "    return obj\r\n",
        "\r\n",
        "\r\n",
        "def save_csv(path, filename, dataframe):\r\n",
        "    print('Saving Dataframe to CSV')\r\n",
        "    path_file = join(path, filename)    \r\n",
        "    dataframe.to_csv(path_file, index=False)\r\n",
        "\r\n",
        "def load_csv(path, filename):\r\n",
        "    print('Loading Dataframe From CSV')\r\n",
        "    path_file = join(path, filename)\r\n",
        "    dataframe = pd.read_csv(path_file)\r\n",
        "    return dataframe"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5arZ4extzh_h"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMIpYZLAqAxY"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "newStopWords = ['type','wild','function','observed','results','different','identified','from','furthermore','process','model','effect','known','suggest','damage','human','rat','rats',\r\n",
        "                'suggested','conclusion','determined','indicate','Moreover','system', 'form', 'treatment','disease','although','conclusion','including','exposure','repair','injury','cell',\r\n",
        "                'functions','compared','interaction','level','demonstrated','studies','substrate','interaction','shown','contrast','presence','important', 'response','cells', 'effect','risk',\r\n",
        "                'induction', 'increase', 'also', 'breaks','treated','showed','used','comet', 'affect', 'may', 'two','study','gene','end','ends','let','double','single','strands','stress',\r\n",
        "                'pathway','role','demonstrate','present','result','use','occur','appear','joining','affect','cause','induce','defect','induced','lead','complex','resistance','calls','ber',\r\n",
        "                'analyzed','include','similar','studied','method','addition','suggesting','background','gene','expression','investigated','promote','types','dna','proteins','protein', 'ner',\r\n",
        "                'high','low','agents','using','strands','cycle','drug','investigate','increased','that','induce','structure','bindings','binding','molecular','mouse','mice','activity',\r\n",
        "                'levels','level','patient','patients','dose','drug','recombination','yeast','strand','mutants','mutations','replications','homologous','genetic','genes','associative','lung',\r\n",
        "                'specific','light','molecular','binding','sequence','lines','cancer','poly','inhibitation','activation','nuclear','regulation','tumor','mismatch','excision','base',\r\n",
        "                'cellular','growth','irridation','tumors','mutations','transription','resistance','site','adducts','dose','enzyme','associated','analysis','species','tissue',\r\n",
        "                'factor','domain','clinical','cancers','liver','assay','irradiated','irradiation','nucleotide','one','damage','increase','resist','mechanism','association','group','groups','aging']\r\n",
        "stopwords.extend(newStopWords)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIGTATj0zTQB"
      },
      "source": [
        "def preprocess(text):\r\n",
        "  text = stemmer.stem(text) \r\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "  words_tokens = tokenizer.tokenize(text) \r\n",
        "  tokens = []  \r\n",
        "  for w in words_tokens:  \r\n",
        "    if w not in stopwords:  \r\n",
        "      tokens.append(w)  \r\n",
        "  tokens = [x for x in tokens if len(x) >= 3]\r\n",
        "  return tokens"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCPuOzmpNS-u"
      },
      "source": [
        "tokens = [preprocess(i) for i in df['Text']]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EblsxpE0l4B8",
        "outputId": "d5061d06-c66b-459e-969c-515913a5e0eb"
      },
      "source": [
        "save_object('/content/drive/MyDrive/models','topic_tokens',tokens)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving Object\n",
            "Save complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpzaq1gNzxfz"
      },
      "source": [
        "# LDA model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlgR5fwTzo9Z",
        "outputId": "e1933d33-37fa-4431-e607-c831dec8487d"
      },
      "source": [
        "# Create Dictionary\r\n",
        "id2word = corpora.Dictionary(tokens)\r\n",
        "# Create Corpus\r\n",
        "texts = tokens\r\n",
        "# Term Document Frequency\r\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\r\n",
        "# View\r\n",
        "print(corpus[:1][0][:30])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 5), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMPIHyjwNOHD"
      },
      "source": [
        "# number of topics\r\n",
        "num_topics = 8\r\n",
        "# Build LDA model\r\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
        "                                       id2word=id2word,\r\n",
        "                                       num_topics=num_topics)\r\n",
        "# Print the Keyword in the 10 topics\r\n",
        "pprint(lda_model.print_topics())\r\n",
        "doc_lda = lda_model[corpus]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er4JnfMisQBU",
        "outputId": "7c26e91d-7f05-44a2-a2c4-fa44d5b01e35"
      },
      "source": [
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.011*\"mutation\" + 0.007*\"syndrome\" + 0.006*\"breast\" + 0.006*\"colorectal\" + '\n",
            "  '0.005*\"atm\" + 0.004*\"hereditary\" + 0.004*\"cases\" + 0.004*\"hnpcc\" + '\n",
            "  '0.004*\"found\" + 0.004*\"polymorphisms\"'),\n",
            " (1,\n",
            "  '0.004*\"telomere\" + 0.004*\"mitochondrial\" + 0.003*\"beta\" + 0.003*\"wound\" + '\n",
            "  '0.003*\"normal\" + 0.003*\"age\" + 0.003*\"control\" + 0.003*\"tgf\" + 0.002*\"bone\" '\n",
            "  '+ 0.002*\"significantly\"'),\n",
            " (2,\n",
            "  '0.014*\"p53\" + 0.008*\"apoptosis\" + 0.006*\"effects\" + 0.006*\"inhibition\" + '\n",
            "  '0.005*\"radiation\" + 0.005*\"parp\" + 0.005*\"inhibitors\" + 0.005*\"mechanisms\" '\n",
            "  '+ 0.004*\"kinase\" + 0.004*\"dependent\"'),\n",
            " (3,\n",
            "  '0.006*\"chromatin\" + 0.005*\"transcription\" + 0.005*\"coli\" + '\n",
            "  '0.004*\"replication\" + 0.004*\"strain\" + 0.004*\"genome\" + 0.003*\"involved\" + '\n",
            "  '0.003*\"strains\" + 0.003*\"mutant\" + 0.003*\"dependent\"'),\n",
            " (4,\n",
            "  '0.006*\"replication\" + 0.005*\"coli\" + 0.004*\"synthesis\" + 0.004*\"lesions\" + '\n",
            "  '0.004*\"reca\" + 0.003*\"polymerase\" + 0.003*\"mutant\" + 0.003*\"escherichia\" + '\n",
            "  '0.003*\"oxidative\" + 0.003*\"containing\"'),\n",
            " (5,\n",
            "  '0.006*\"genome\" + 0.006*\"methylation\" + 0.006*\"mgmt\" + 0.005*\"mutation\" + '\n",
            "  '0.005*\"crispr\" + 0.005*\"msi\" + 0.004*\"cas9\" + 0.004*\"promoter\" + '\n",
            "  '0.004*\"microsatellite\" + 0.004*\"colorectal\"'),\n",
            " (6,\n",
            "  '0.010*\"cisplatin\" + 0.007*\"survival\" + 0.006*\"radiation\" + '\n",
            "  '0.005*\"chemotherapy\" + 0.004*\"therapy\" + 0.004*\"brca1\" + 0.004*\"resistant\" '\n",
            "  '+ 0.004*\"platinum\" + 0.003*\"stem\" + 0.003*\"apoptosis\"'),\n",
            " (7,\n",
            "  '0.008*\"dsb\" + 0.008*\"chromosome\" + 0.006*\"dsbs\" + 0.005*\"radiation\" + '\n",
            "  '0.005*\"chromosomal\" + 0.005*\"break\" + 0.004*\"nhej\" + 0.004*\"non\" + '\n",
            "  '0.004*\"formation\" + 0.003*\"dependent\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTnIQVA0qqME"
      },
      "source": [
        "## Analysis of Results\r\n",
        "\r\n",
        "### What is the Dominant topic and its percentage contribution in each document?\r\n",
        "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\r\n",
        "\r\n",
        "This way, you will know which document belongs predominantly to which topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lou5_sEXwxpE"
      },
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus = corpus, texts= texts):\r\n",
        "    # Init output\r\n",
        "    sent_topics_df = pd.DataFrame()\r\n",
        "\r\n",
        "    # Get main topic in each document\r\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\r\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \r\n",
        "        # print(row)\r\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\r\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\r\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\r\n",
        "            if j == 0:  # => dominant topic\r\n",
        "                wp = ldamodel.show_topic(topic_num)\r\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\r\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\r\n",
        "            else:\r\n",
        "                break\r\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\r\n",
        "\r\n",
        "    # Add original text to the end of the output\r\n",
        "    contents = pd.Series(texts)\r\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\r\n",
        "    return sent_topics_df\r\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_00NIYxMDw"
      },
      "source": [
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "aNyrP_2K1WC_",
        "outputId": "5cfa51e1-0bf7-4804-e0e7-6d2d25d7705f"
      },
      "source": [
        "# Format\r\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\r\n",
        "df_dominant_topic.columns = ['PMID', 'Dominant Topic', 'Topic % Contribution', 'Keywords', 'Text']\r\n",
        "df_dominant_topic['PMID'] = df['PMID']\r\n",
        "df_dominant_topic.head(8)\r\n",
        "df_dominant_topic.to_csv('dominant_topics.csv')\r\n",
        "#save_csv('/content/drive/MyDrive/data','dominant_topics',df_dominant_topic)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMID</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Topic % Contribution</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33581499</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4045</td>\n",
              "      <td>telomere, mitochondrial, beta, wound, normal, ...</td>\n",
              "      <td>[pcna, focus, replication, alternative, length...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33581339</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.3878</td>\n",
              "      <td>chromatin, transcription, coli, replication, s...</td>\n",
              "      <td>[compositional, variability, mutation, spectra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33580188</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>chromatin, transcription, coli, replication, s...</td>\n",
              "      <td>[histone, variant, h2a, h2b, dimers, spontaneo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33580181</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.3826</td>\n",
              "      <td>cisplatin, survival, radiation, chemotherapy, ...</td>\n",
              "      <td>[hypermutated, phenotype, gliosarcoma, spinal,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33580144</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.6327</td>\n",
              "      <td>p53, apoptosis, effects, inhibition, radiation...</td>\n",
              "      <td>[arsenic, hexoxide, differential, effects, pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33579976</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.4223</td>\n",
              "      <td>cisplatin, survival, radiation, chemotherapy, ...</td>\n",
              "      <td>[persistent, stag2, mutation, despite, multimo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>33579974</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5129</td>\n",
              "      <td>mutation, syndrome, breast, colorectal, atm, h...</td>\n",
              "      <td>[somatic, aberrations, benign, breast, subsequ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33579938</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.7292</td>\n",
              "      <td>genome, methylation, mgmt, mutation, crispr, m...</td>\n",
              "      <td>[whole, genome, sequencing, data, derive, defi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PMID  ...                                               Text\n",
              "0  33581499  ...  [pcna, focus, replication, alternative, length...\n",
              "1  33581339  ...  [compositional, variability, mutation, spectra...\n",
              "2  33580188  ...  [histone, variant, h2a, h2b, dimers, spontaneo...\n",
              "3  33580181  ...  [hypermutated, phenotype, gliosarcoma, spinal,...\n",
              "4  33580144  ...  [arsenic, hexoxide, differential, effects, pro...\n",
              "5  33579976  ...  [persistent, stag2, mutation, despite, multimo...\n",
              "6  33579974  ...  [somatic, aberrations, benign, breast, subsequ...\n",
              "7  33579938  ...  [whole, genome, sequencing, data, derive, defi...\n",
              "\n",
              "[8 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Dataframe to CSV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuh7bKQC4Xiu"
      },
      "source": [
        "unique = set(df_dominant_topic['Keywords'].str.split(' ').sum())\r\n",
        "\r\n",
        "print(list(sorted(unique)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Latk48AG6hI2"
      },
      "source": [
        "### The most representative sentence for each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz168mgY6gwG"
      },
      "source": [
        "# Display setting to show more characters in column\r\n",
        "pd.options.display.max_colwidth = 100\r\n",
        "\r\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\r\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\r\n",
        "\r\n",
        "for i, grp in sent_topics_outdf_grpd:\r\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \r\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \r\n",
        "                                            axis=0)\r\n",
        "\r\n",
        "# Reset Index    \r\n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\r\n",
        "\r\n",
        "# Format\r\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\r\n",
        "\r\n",
        "# Show\r\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vepcv0ZUxsL4"
      },
      "source": [
        "### Visualize The topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "VKzPZmjIqp2N",
        "outputId": "6bdb23c5-7a81-442e-d6b9-5e18a13cf831"
      },
      "source": [
        "# Visualize the topics\r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "LDAvis_data_filepath = join('./results/ldavis_prepared_'+str(num_topics))\r\n",
        "# # this is a bit time consuming - make the if statement True\r\n",
        "# # if you want to execute visualization prep yourself\r\n",
        "if 1 == 1:\r\n",
        "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\r\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\r\n",
        "        pickle.dump(LDAvis_prepared, f)\r\n",
        "# load the pre-prepared pyLDAvis data from disk\r\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\r\n",
        "    LDAvis_prepared = pickle.load(f)\r\n",
        "pyLDAvis.save_html(LDAvis_prepared, '/content/drive/MyDrive/models'+ str(num_topics) +'.html')\r\n",
        "LDAvis_prepared"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-207b09530ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_data_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# load the pre-prepared pyLDAvis data from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/ldavis_prepared_8'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdLNpb8rSkjB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}